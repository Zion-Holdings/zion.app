#!/usr/bin/env node

const fs = require('fs');
const path = require('path');

const ROOT = process.cwd();
const PAGES_DIR = path.join(ROOT, 'pages');
<<<<<<< HEAD
const PUBLIC_DIR = path.join(ROOT, 'public');
const REPORT_DIR = path.join(ROOT, 'data', 'reports', 'internal-links');

function ensureDir(p) { if (!fs.existsSync(p)) fs.mkdirSync(p, { recursive: true }); }

function listPageFiles(dir) {
  const out = [];
  const ignore = new Set(['.next','node_modules','api']);
  (function walk(d){
    let items = [];
    try { items = fs.readdirSync(d, { withFileTypes: true }); } catch { return; }
    for (const it of items) {
      const full = path.join(d, it.name);
      if (it.isDirectory()) {
        if (!ignore.has(it.name) && !it.name.startsWith('_') && !it.name.startsWith('.')) walk(full);
      } else if (it.isFile()) {
        if (/\.(tsx|jsx|mdx|js|ts)$/.test(it.name) && !it.name.startsWith('_')) out.push(full);
      }
    }
  })(dir);
  return out;
}

function getRoutesFromPages(dir, base = '') {
  let routes = [];
  let entries = [];
  try { entries = fs.readdirSync(dir, { withFileTypes: true }); } catch { return []; }
  for (const e of entries) {
    if (e.name.startsWith('_')) continue;
    const full = path.join(dir, e.name);
    if (e.isDirectory()) {
      if (e.name === 'api') continue;
      routes = routes.concat(getRoutesFromPages(full, base + '/' + e.name));
    } else if (e.isFile()) {
      if (!e.name.match(/\.(tsx|jsx|mdx|js|ts)$/)) continue;
      const name = e.name.replace(/\.(tsx|jsx|mdx|js|ts)$/, '');
      if (name === 'index') routes.push(base || '/');
      else if (!name.startsWith('[')) routes.push(base + '/' + name);
=======
const REPORT_JSON_DIR = path.join(ROOT, 'data', 'reports', 'internal-links');
const REPORT_HTML_DIR = path.join(ROOT, 'public', 'reports', 'links');

function ensureDir(dirPath) {
  fs.mkdirSync(dirPath, { recursive: true });
}

function* walkFiles(startDir, exts = ['.tsx', '.ts', '.jsx', '.js', '.mdx']) {
  if (!fs.existsSync(startDir)) return;
  const stack = [startDir];
  while (stack.length) {
    const current = stack.pop();
    let entries = [];
    try { entries = fs.readdirSync(current, { withFileTypes: true }); } catch { continue; }
    for (const entry of entries) {
      const full = path.join(current, entry.name);
      if (entry.isDirectory()) {
        if (entry.name === 'api' || entry.name === 'node_modules' || entry.name.startsWith('.')) continue;
        stack.push(full);
      } else if (entry.isFile()) {
        if (exts.includes(path.extname(entry.name))) yield full;
      }
    }
  }
}

function getRoutes(dir, base = '') {
  if (!fs.existsSync(dir)) return [];
  const entries = fs.readdirSync(dir, { withFileTypes: true });
  const routes = [];
  for (const e of entries) {
    if (e.name.startsWith('_')) continue; // skip Next internals
    if (e.isDirectory()) {
      if (e.name === 'api') continue;
      routes.push(...getRoutes(path.join(dir, e.name), base + '/' + e.name));
    } else if (e.isFile()) {
      if (!e.name.match(/\.(tsx|jsx|mdx|js|ts)$/)) continue;
      const name = e.name.replace(/\.(tsx|jsx|mdx|js|ts)$/, '');
      if (name === 'index') {
        routes.push(base || '/');
      } else if (!name.startsWith('[')) {
        routes.push(base + '/' + name);
      }
>>>>>>> origin/cursor/crawl-and-fix-site-links-and-pages-c7eb
    }
  }
  return Array.from(new Set(routes)).sort();
}

<<<<<<< HEAD
function extractInternalLinks(src) {
  const links = new Set();
  // href="/path" and href='/path' and href={"/path"}
  const re1 = /href\s*=\s*(?:\{\s*)?["'](\/[^"'#? ]*)["']/g;
  let m;
  while ((m = re1.exec(src))) {
    links.add(m[1]);
  }
  // Next.js <Link href="/path">
  const re2 = /<Link\s+[^>]*href\s*=\s*(?:\{\s*)?["'](\/[^"'#? ]*)["']/g;
  while ((m = re2.exec(src))) {
    links.add(m[1]);
  }
  // Markdown [text](/path)
  const re3 = /\[[^\]]+\]\((\/[^(\)\s#?]+)\)/g;
  while ((m = re3.exec(src))) {
    links.add(m[1]);
=======
function extractInternalLinks(text) {
  const links = new Set();
  // Match href="/path" or href={'/path'} or Link href="/path"
  const regex = /href\s*=\s*{?\s*["'`](\/[\w\-\/\.]*)(?:#[^"'`}]*)?["'`}\)]/g;
  let m;
  while ((m = regex.exec(text)) !== null) {
    const url = m[1];
    if (!url) continue;
    if (url.startsWith('//')) continue; // protocol-relative external
    if (url.startsWith('/api')) continue; // ignore API routes
    if (url.startsWith('/_next')) continue; // ignore Next internals
    links.add(url.replace(/\/$/, '') || '/');
>>>>>>> origin/cursor/crawl-and-fix-site-links-and-pages-c7eb
  }
  return Array.from(links);
}

<<<<<<< HEAD
function normalizePath(p) {
  if (!p.startsWith('/')) return p;
  // remove trailing slash except root
  if (p.length > 1 && p.endsWith('/')) return p.slice(0, -1);
  return p;
}

function publicAssetExists(p) {
  // direct match in public (e.g., /reports/ai-trends/index.html)
  const clean = p.replace(/^\//, '');
  const direct = path.join(PUBLIC_DIR, clean);
  if (fs.existsSync(direct)) return true;
  // folder index.html
  const idx = path.join(PUBLIC_DIR, clean, 'index.html');
  if (fs.existsSync(idx)) return true;
  return false;
}

async function main() {
  ensureDir(REPORT_DIR);
  if (!fs.existsSync(PAGES_DIR)) {
    console.error('pages/ directory not found.');
    process.exit(1);
  }

  const routes = new Set(getRoutesFromPages(PAGES_DIR).map(normalizePath));
  const files = listPageFiles(PAGES_DIR);

  const graph = {}; // route/file -> outgoing links
  const missing = []; // { sourceFile, href }

  for (const file of files) {
    let src = '';
    try { src = fs.readFileSync(file, 'utf8'); } catch { continue; }
    const links = extractInternalLinks(src).map(normalizePath);
    graph[file.replace(ROOT + path.sep, '')] = links;
    for (const href of links) {
      // Treat Netlify Functions as valid endpoints in production
      if (href.startsWith('/.netlify/functions/')) continue;
      const inRoutes = routes.has(href);
      const inPublic = publicAssetExists(href);
      if (!inRoutes && !inPublic) {
        missing.push({ sourceFile: file.replace(ROOT + path.sep, ''), href });
      }
    }
  }

  const summary = {
    timestamp: new Date().toISOString(),
    totalFilesScanned: files.length,
    totalRoutes: routes.size,
    missingLinks: missing.length,
    routes: Array.from(routes),
    missing,
  };

  const latestPath = path.join(REPORT_DIR, 'latest.json');
  fs.writeFileSync(latestPath, JSON.stringify(summary, null, 2));
  const datedPath = path.join(REPORT_DIR, `internal-links-${Date.now()}.json`);
  fs.writeFileSync(datedPath, JSON.stringify(summary, null, 2));

  console.log(`Internal link crawl complete. Files: ${files.length}. Missing: ${missing.length}. Report: ${latestPath}`);
}

main().catch((e) => { console.error(e); process.exit(1); });
=======
function readFileSafe(file) {
  try { return fs.readFileSync(file, 'utf8'); } catch { return ''; }
}

function discoverRedirectMap() {
  // Simple static discovery from next.config.js known redirects
  const configPath = path.join(ROOT, 'next.config.js');
  const map = new Map();
  const text = readFileSafe(configPath);
  // Look for patterns like { source: '/front', destination: '/', permanent: true }
  const re = /source:\s*['"]([^'"]+)['"],\s*destination:\s*['"]([^'"]+)['"]/g;
  let m;
  while ((m = re.exec(text)) !== null) {
    map.set(m[1], m[2]);
  }
  return map;
}

function renderHtmlReport(results) {
  const rows = results.map(r => `
      <tr>
        <td>${r.file.replace(ROOT + '/', '')}</td>
        <td>${r.link}</td>
        <td>${r.status}</td>
        <td>${r.suggestion || ''}</td>
      </tr>`).join('\n');
  return `<!DOCTYPE html>
<html><head><meta charset="utf-8"/><title>Internal Link Report</title>
<style>
body{font-family:system-ui,Segoe UI,Roboto,Inter,sans-serif;margin:24px}
 table{border-collapse:collapse;width:100%}
 th,td{border:1px solid #ddd;padding:8px;font-size:14px}
 th{background:#f3f4f6;text-align:left}
 .ok{color:#065f46}
 .bad{color:#991b1b}
</style></head>
<body>
  <h1>Internal Link Report</h1>
  <p>Validated internal links found in pages. Broken links include a suggestion when a redirect is known.</p>
  <table>
    <thead><tr><th>File</th><th>Link</th><th>Status</th><th>Suggestion</th></tr></thead>
    <tbody>
${rows || '<tr><td colspan="4">No issues found.</td></tr>'}
    </tbody>
  </table>
</body></html>`;
}

async function main() {
  const validRoutes = new Set(getRoutes(PAGES_DIR).map(r => (r === '' ? '/' : r)));
  // Normalize: include root without trailing slash
  const normalizedRoutes = new Set(Array.from(validRoutes, r => r === '/' ? '/' : r.replace(/\/$/, '')));
  const redirectMap = discoverRedirectMap();

  const issues = [];
  const scanned = [];

  for (const file of walkFiles(PAGES_DIR)) {
    const text = readFileSafe(file);
    if (!text) continue;
    const links = extractInternalLinks(text);
    if (!links.length) continue;
    for (const link of links) {
      const normalizedLink = link === '/' ? '/' : link.replace(/\/$/, '');
      let status = 'ok';
      let suggestion = null;
      if (!normalizedRoutes.has(normalizedLink)) {
        status = 'missing';
        // try redirect map
        if (redirectMap.has(normalizedLink)) {
          suggestion = redirectMap.get(normalizedLink);
        } else if (redirectMap.has(normalizedLink.replace(/\/$/, ''))) {
          suggestion = redirectMap.get(normalizedLink.replace(/\/$/, ''));
        }
        issues.push({ file, link: normalizedLink, status, suggestion });
      }
      scanned.push({ file, link: normalizedLink, status, suggestion });
    }
  }

  ensureDir(REPORT_JSON_DIR);
  ensureDir(REPORT_HTML_DIR);

  const payload = {
    generatedAt: new Date().toISOString(),
    totalLinks: scanned.length,
    broken: issues.length,
    issues,
  };

  fs.writeFileSync(path.join(REPORT_JSON_DIR, `internal-links-${Date.now()}.json`), JSON.stringify(payload, null, 2));
  fs.writeFileSync(path.join(REPORT_JSON_DIR, `latest.json`), JSON.stringify(payload, null, 2));

  const html = renderHtmlReport(issues);
  fs.writeFileSync(path.join(REPORT_HTML_DIR, 'index.html'), html);

  console.log(`Internal link crawl complete. Scanned: ${scanned.length}. Broken: ${issues.length}.`);
}

main().catch((e) => { console.error(e); process.exit(1); });
>>>>>>> origin/cursor/crawl-and-fix-site-links-and-pages-c7eb
