#!/usr/bin/env node

const fs = require('fs');
const path = require('path');
<<<<<<< HEAD
const axios = require('axios');
const cheerio = require('cheerio');

const REPORT_DIR = path.join(process.cwd(), 'data', 'reports', 'site-links');

function ensureDir(dirPath) {
  fs.mkdirSync(dirPath, { recursive: true });
}

function getBaseUrl() {
  // Prefer explicit base from env, then Netlify envs, then fallback to localhost
  const explicit = process.env.CRAWL_BASE_URL || process.env.SITEMAP_BASE_URL;
  const netlify = process.env.DEPLOY_PRIME_URL || process.env.DEPLOY_URL || process.env.URL;
  const fallback = 'http://localhost:3000';
  const base = (explicit || netlify || fallback).trim().replace(/\/$/, '');
  if (!/^https?:\/\//i.test(base)) return 'http://localhost:3000';
  return base;
}

function normalizePath(href) {
  try {
    if (!href) return null;
    if (href.startsWith('mailto:') || href.startsWith('tel:')) return null;
    if (href.startsWith('#')) return null;
    // Absolute URL
    if (/^https?:\/\//i.test(href)) {
      const u = new URL(href);
      return u.pathname.replace(/\/index$/, '/') || '/';
    }
    // Root-relative
    if (href.startsWith('/')) {
      return href.replace(/\/index$/, '/') || '/';
    }
    return null;
  } catch {
    return null;
  }
}

async function fetchPage(url) {
  try {
    const res = await axios.get(url, { timeout: 15000, validateStatus: () => true });
    return { status: res.status, html: res.data };
  } catch (e) {
    return { status: 0, error: e.message };
  }
}

async function crawlSite() {
  const baseUrl = getBaseUrl();
  const origin = new URL(baseUrl).origin;

  const queue = [{ path: '/', depth: 0 }];
  const visited = new Set();
  const maxDepth = Number(process.env.CRAWL_MAX_DEPTH || 4);
  const maxPages = Number(process.env.CRAWL_MAX_PAGES || 200);

  /** @type {Record<string, { url: string, status: number, ok: boolean, links: string[] }>} */
  const pages = {};
  const discoveredInternal = new Set(['/']);

  while (queue.length && visited.size < maxPages) {
    const current = queue.shift();
    if (!current) break;
    const { path: p, depth } = current;
    if (visited.has(p)) continue;
    visited.add(p);

    const fullUrl = origin + p;
    const res = await fetchPage(fullUrl);
    const ok = res.status >= 200 && res.status < 400;

    let links = [];
    if (ok && res.html) {
      try {
        const $ = cheerio.load(res.html);
        const anchors = Array.from($('a')).map((a) => $(a).attr('href'));
        for (const href of anchors) {
          const np = normalizePath(href);
          if (!np) continue;
          // same-origin internal only
          if (np.startsWith('/')) {
            links.push(np);
            if (!discoveredInternal.has(np)) {
              discoveredInternal.add(np);
              if (depth + 1 <= maxDepth) queue.push({ path: np, depth: depth + 1 });
            }
          }
        }
      } catch {
        // ignore parse errors
      }
    }

    pages[p] = { url: fullUrl, status: res.status, ok, links: Array.from(new Set(links)).sort() };
  }

  // Verify all discovered internal paths resolve
  const missingInternalPaths = [];
  for (const p of Array.from(discoveredInternal)) {
    if (!pages[p]) {
      const fullUrl = origin + p;
      const res = await fetchPage(fullUrl);
      const ok = res.status >= 200 && res.status < 400;
      if (!ok) missingInternalPaths.push({ path: p, status: res.status || 0 });
    } else if (!pages[p].ok) {
      missingInternalPaths.push({ path: p, status: pages[p].status || 0 });
    }
  }

  const report = {
    generatedAt: new Date().toISOString(),
    baseUrl: origin,
    stats: { pagesCrawled: Object.keys(pages).length, discoveredInternal: discoveredInternal.size, missing: missingInternalPaths.length },
    pages,
    missingInternalPaths: missingInternalPaths.sort((a, b) => a.path.localeCompare(b.path)),
  };

  ensureDir(REPORT_DIR);
  const stamp = Date.now();
  const outFile = path.join(REPORT_DIR, `site-links-${stamp}.json`);
  fs.writeFileSync(outFile, JSON.stringify(report, null, 2));
  fs.writeFileSync(path.join(REPORT_DIR, 'latest.json'), JSON.stringify(report, null, 2));
  console.log(`Crawl complete. Pages: ${Object.keys(pages).length}. Missing internal: ${missingInternalPaths.length}. Report: ${outFile}`);
}

crawlSite().catch((e) => { console.error(e); process.exit(1); });
=======

const ROOT = process.cwd();
const PAGES_DIR = path.join(ROOT, 'pages');
const PUBLIC_DIR = path.join(ROOT, 'public');
const REPORT_DIR = path.join(ROOT, 'data', 'reports', 'internal-links');

function ensureDir(p) { if (!fs.existsSync(p)) fs.mkdirSync(p, { recursive: true }); }

function listPageFiles(dir) {
  const out = [];
  const ignore = new Set(['.next','node_modules','api']);
  (function walk(d){
    let items = [];
    try { items = fs.readdirSync(d, { withFileTypes: true }); } catch { return; }
    for (const it of items) {
      const full = path.join(d, it.name);
      if (it.isDirectory()) {
        if (!ignore.has(it.name) && !it.name.startsWith('_') && !it.name.startsWith('.')) walk(full);
      } else if (it.isFile()) {
        if (/\.(tsx|jsx|mdx|js|ts)$/.test(it.name) && !it.name.startsWith('_')) out.push(full);
      }
    }
  })(dir);
  return out;
}

function getRoutesFromPages(dir, base = '') {
  let routes = [];
  let entries = [];
  try { entries = fs.readdirSync(dir, { withFileTypes: true }); } catch { return []; }
  for (const e of entries) {
    if (e.name.startsWith('_')) continue;
    const full = path.join(dir, e.name);
    if (e.isDirectory()) {
      if (e.name === 'api') continue;
      routes = routes.concat(getRoutesFromPages(full, base + '/' + e.name));
    } else if (e.isFile()) {
      if (!e.name.match(/\.(tsx|jsx|mdx|js|ts)$/)) continue;
      const name = e.name.replace(/\.(tsx|jsx|mdx|js|ts)$/, '');
      if (name === 'index') routes.push(base || '/');
      else if (!name.startsWith('[')) routes.push(base + '/' + name);
    }
  }
  return Array.from(new Set(routes)).sort();
}

function extractInternalLinks(src) {
  const links = new Set();
  // href="/path" and href='/path' and href={"/path"}
  const re1 = /href\s*=\s*(?:\{\s*)?["'](\/[^"'#? ]*)["']/g;
  let m;
  while ((m = re1.exec(src))) {
    links.add(m[1]);
  }
  // Next.js <Link href="/path">
  const re2 = /<Link\s+[^>]*href\s*=\s*(?:\{\s*)?["'](\/[^"'#? ]*)["']/g;
  while ((m = re2.exec(src))) {
    links.add(m[1]);
  }
  // Markdown [text](/path)
  const re3 = /\[[^\]]+\]\((\/[^(\)\s#?]+)\)/g;
  while ((m = re3.exec(src))) {
    links.add(m[1]);
  }
  return Array.from(links);
}

function normalizePath(p) {
  if (!p.startsWith('/')) return p;
  // remove trailing slash except root
  if (p.length > 1 && p.endsWith('/')) return p.slice(0, -1);
  return p;
}

function publicAssetExists(p) {
  // direct match in public (e.g., /reports/ai-trends/index.html)
  const clean = p.replace(/^\//, '');
  const direct = path.join(PUBLIC_DIR, clean);
  if (fs.existsSync(direct)) return true;
  // folder index.html
  const idx = path.join(PUBLIC_DIR, clean, 'index.html');
  if (fs.existsSync(idx)) return true;
  return false;
}

async function main() {
  ensureDir(REPORT_DIR);
  if (!fs.existsSync(PAGES_DIR)) {
    console.error('pages/ directory not found.');
    process.exit(1);
  }

  const routes = new Set(getRoutesFromPages(PAGES_DIR).map(normalizePath));
  const files = listPageFiles(PAGES_DIR);

  const graph = {}; // route/file -> outgoing links
  const missing = []; // { sourceFile, href }

  for (const file of files) {
    let src = '';
    try { src = fs.readFileSync(file, 'utf8'); } catch { continue; }
    const links = extractInternalLinks(src).map(normalizePath);
    graph[file.replace(ROOT + path.sep, '')] = links;
    for (const href of links) {
      // Treat Netlify Functions as valid endpoints in production
      if (href.startsWith('/.netlify/functions/')) continue;
      const inRoutes = routes.has(href);
      const inPublic = publicAssetExists(href);
      if (!inRoutes && !inPublic) {
        missing.push({ sourceFile: file.replace(ROOT + path.sep, ''), href });
      }
    }
  }

  const summary = {
    timestamp: new Date().toISOString(),
    totalFilesScanned: files.length,
    totalRoutes: routes.size,
    missingLinks: missing.length,
    routes: Array.from(routes),
    missing,
  };

  const latestPath = path.join(REPORT_DIR, 'latest.json');
  fs.writeFileSync(latestPath, JSON.stringify(summary, null, 2));
  const datedPath = path.join(REPORT_DIR, `internal-links-${Date.now()}.json`);
  fs.writeFileSync(datedPath, JSON.stringify(summary, null, 2));

  console.log(`Internal link crawl complete. Files: ${files.length}. Missing: ${missing.length}. Report: ${latestPath}`);
}

main().catch((e) => { console.error(e); process.exit(1); });
>>>>>>> origin/cursor/crawl-and-fix-site-links-and-pages-2625
